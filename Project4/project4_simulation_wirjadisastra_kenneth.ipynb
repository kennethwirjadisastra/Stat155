{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "from joblib import Parallel, delayed\n",
    "import datetime\n",
    "import os"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First we will define a function to set the seed for reproducability and set the device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True  # ensures deterministic algorithms\n",
    "    torch.backends.cudnn.benchmark = False     # can slow down, but more reproducible\n",
    "\n",
    "set_seed(42)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next we can define the RNN class and TimeSeries datasets from [Project 3](https://github.com/kennethwirjadisastra/Stat155/tree/main/Project3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeatherRNN(nn.Module):\n",
    "  def __init__(self, input_size:int, hidden_size:int, output_size:int, num_layers:int=1, rnn_type:str='RNN', dropout:float=0.0):\n",
    "    super().__init__()\n",
    "\n",
    "    # potential RNN classes\n",
    "    rnn_options = {\"RNN\": nn.RNN,\n",
    "                   \"LSTM\": nn.LSTM,\n",
    "                   \"GRU\": nn.GRU}\n",
    "\n",
    "    if rnn_type not in rnn_options:\n",
    "      raise ValueError(f'rnn_type must be one of {list(rnn_options.keys())}')\n",
    "\n",
    "    # force dropout to be 0 if num_layers == 1\n",
    "    dropout = dropout if num_layers > 1 else 0.0\n",
    "\n",
    "    self.rnn_type = rnn_type\n",
    "\n",
    "    # define the rnn\n",
    "    self.rnn = rnn_options[rnn_type](input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)\n",
    "\n",
    "    # fully connected linear layers\n",
    "    self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "  def forward(self, x, h0=None):\n",
    "    batch_size = x.size(0)\n",
    "    out, _ = self.rnn(x) if h0 is None else self.rnn(x, h0)\n",
    "    out = self.fc(out[:,-1,:])\n",
    "    return out, _\n",
    "  \n",
    "\n",
    "class TimeSeries(Dataset):\n",
    "  def __init__(self, data, seq_len):\n",
    "    self.data = torch.tensor(data, dtype=torch.float32)\n",
    "    self.seq_len = seq_len\n",
    "\n",
    "  def __len__(self):\n",
    "    # subtract self.seq_len to account for data where previous sequence is unknown\n",
    "    return len(self.data) - self.seq_len\n",
    "\n",
    "  def __getitem__(self, start_idx):\n",
    "    end_idx = start_idx + self.seq_len\n",
    "\n",
    "    # generate the sequence of size seq_len and the target that follows directly after\n",
    "    X_seq = self.data[start_idx:end_idx].unsqueeze(-1) # (seq_len x 1)\n",
    "    y = self.data[end_idx].unsqueeze(-1) # (1,)\n",
    "\n",
    "    return X_seq, y"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We will also reuse the same functions defined in [Project 3](https://github.com/kennethwirjadisastra/Stat155/tree/main/Project3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(stations):\n",
    "    # create array to store each dataframe created\n",
    "    df_dict = {}         # each value in df_dict will be an dictionary with values of pd.DataFrame objects\n",
    "                         # the key of each dataframe determines the type of dataset as shown below\n",
    "\n",
    "    dataset_types = {\n",
    "        0: \"train\",      # first element\n",
    "        1: \"validation\", # second element\n",
    "        2: \"test\"        # third element\n",
    "    }\n",
    "\n",
    "    # recover necessary data from each csv file\n",
    "    for station in stations:\n",
    "\n",
    "        for j in dataset_types.keys():\n",
    "            df = pd.read_csv(f'../Project1/{dataset_types[j]}_data/{station}_weather_{dataset_types[j]}.csv')\n",
    "\n",
    "            df[\"DATE\"] = pd.to_datetime(df[\"DATE\"])\n",
    "        \n",
    "            df.set_index(\"DATE\", inplace=True)\n",
    "        \n",
    "            df = pd.DataFrame(df[\"TMAX\"])\n",
    "\n",
    "            # add dataframe to array\n",
    "            df_dict[station] = [df] if station not in df_dict.keys() else df_dict[station] + [df]\n",
    "\n",
    "    return df_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(df_arr, transformation):\n",
    "    # array to store normalized data\n",
    "    norm_df_arr = []\n",
    "\n",
    "    # normalize data and append to array\n",
    "    for df in df_arr:\n",
    "        df[\"NORM\"] = transformation(df[\"TMAX\"].values.reshape(-1,1))\n",
    "        norm_df_arr.append(df)\n",
    "    \n",
    "    return norm_df_arr\n",
    "\n",
    "def load_data(data_norm, seq_len, batch_size=32, shuffle=False, drop_last=True):\n",
    "    dataset = TimeSeries(data_norm.values, seq_len=seq_len)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, drop_last=True)\n",
    "\n",
    "    return dataset, dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_RNN(model, train_dataloader, validation_dataloader, num_epochs=50, \n",
    "              criterion=nn.MSELoss(), optimizer=optim.Adam, eta=1.e-3, device=device):\n",
    "    \n",
    "    model = model.to(device)\n",
    "\n",
    "    training_loss = np.empty(num_epochs) * np.nan\n",
    "    validation_loss = np.empty(num_epochs) * np.nan\n",
    "\n",
    "    optimizer = optimizer(model.parameters(), lr=eta)\n",
    "\n",
    "    #print(\"Training ...\")\n",
    "\n",
    "    #start_time = time.time()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # training and validation loss for each epoch\n",
    "        train_epoch_loss = 0\n",
    "        validation_epoch_loss = 0\n",
    "\n",
    "        # reset hidden state after each epoch\n",
    "        hidden = None\n",
    "\n",
    "        model.train()\n",
    "        for x_batch, y_batch in train_dataloader:\n",
    "            if hidden is not None:\n",
    "                if model.rnn_type == \"LSTM\":\n",
    "                    hidden = (hidden[0].detach(), hidden[1].detach())\n",
    "                else:\n",
    "                    hidden = hidden.detach()\n",
    "\n",
    "            out, hidden = model(x_batch, hidden)\n",
    "            loss = criterion(out, y_batch)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_epoch_loss += loss.item()\n",
    "        \n",
    "        # average loss over each batch\n",
    "        train_epoch_loss /= len(train_dataloader)\n",
    "\n",
    "        # save losses for plotting later\n",
    "        training_loss[epoch] = train_epoch_loss\n",
    "\n",
    "        \n",
    "        model.eval()\n",
    "        # reset hidden state\n",
    "        hidden = None\n",
    "        with torch.no_grad():\n",
    "            for x_batch, y_batch in validation_dataloader:\n",
    "                x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "                if hidden is not None:\n",
    "                    if model.rnn_type == \"LSTM\":\n",
    "                        hiden = (hidden[0].detach(), hidden[1].detach())\n",
    "                    else:\n",
    "                        hidden = hidden.detach()\n",
    "\n",
    "                out, hidden = model(x_batch, hidden)\n",
    "                loss = criterion(out, y_batch)\n",
    "                validation_epoch_loss += loss.item()\n",
    "            \n",
    "            validation_epoch_loss /= len(validation_dataloader)\n",
    "        \n",
    "        validation_loss[epoch] = validation_epoch_loss\n",
    "\n",
    "        if epoch % 10 == 9:\n",
    "            print(f'Epoch {epoch+1}: \\n\\tTraining Loss = {train_epoch_loss}\\n\\tValidation Loss = {validation_epoch_loss}')\n",
    "\n",
    "    #end_time = time.time()\n",
    "\n",
    "    #print(\"Done!\")\n",
    "    #print(f'Total time (seconds): {end_time-start_time}')\n",
    "\n",
    "    return training_loss, validation_loss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we can load the training, validation, and testing datasets for each of the four stations into a dictionary named `datasets`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "stations = [\"APA\", \"central_park\", \"DEN\", \"water_dept\"]\n",
    "dataframes = read_data(stations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "\n",
    "for station in stations:\n",
    "    dataframes[station] = normalize(dataframes[station], scaler.fit_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 7\n",
    "\n",
    "dataset_types = {\n",
    "        0: \"train\",      # first element\n",
    "        1: \"validation\", # second element\n",
    "        2: \"test\"        # third element\n",
    "    }\n",
    "\n",
    "loaders = {}\n",
    "\n",
    "for station, (train_data, validation_data, test_data) in dataframes.items():\n",
    "    for i in range(len(dataset_types)):\n",
    "        dataset_type = dataset_types[i]\n",
    "\n",
    "        df = dataframes[station][i][\"NORM\"]\n",
    "\n",
    "        _, dataloader = load_data(df, seq_len)\n",
    "\n",
    "        if station not in loaders.keys():\n",
    "            loaders[station] = {dataset_type: dataloader}\n",
    "        else:\n",
    "            loaders[station][dataset_type] = dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_date = datetime.date.today()\n",
    "\n",
    "year = current_date.year\n",
    "month = current_date.month\n",
    "date = current_date.day\n",
    "\n",
    "\n",
    "os.makedirs(f'{year}_{month}_{date}_model_save', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/360 models complete\n",
      "20/360 models complete\n",
      "30/360 models complete\n",
      "40/360 models complete\n",
      "50/360 models complete\n",
      "60/360 models complete\n",
      "70/360 models complete\n",
      "80/360 models complete\n",
      "90/360 models complete\n",
      "100/360 models complete\n",
      "110/360 models complete\n",
      "120/360 models complete\n",
      "130/360 models complete\n",
      "140/360 models complete\n",
      "150/360 models complete\n",
      "160/360 models complete\n",
      "170/360 models complete\n",
      "180/360 models complete\n",
      "190/360 models complete\n",
      "200/360 models complete\n",
      "210/360 models complete\n",
      "220/360 models complete\n",
      "230/360 models complete\n",
      "240/360 models complete\n",
      "250/360 models complete\n",
      "260/360 models complete\n",
      "270/360 models complete\n",
      "280/360 models complete\n",
      "290/360 models complete\n",
      "300/360 models complete\n",
      "310/360 models complete\n",
      "320/360 models complete\n",
      "330/360 models complete\n",
      "340/360 models complete\n",
      "350/360 models complete\n",
      "360/360 models complete\n",
      "Time Elapsed: 1139.811\n"
     ]
    }
   ],
   "source": [
    "def train_member(model, train_loader, val_loader, num_epochs):\n",
    "    train_loss, val_loss = train_RNN(model, train_loader, val_loader, num_epochs=num_epochs)\n",
    "    return train_loss, val_loss\n",
    "\n",
    "\n",
    "model_types = [\"RNN\", \"LSTM\", \"GRU\"]\n",
    "hidden_sizes = [16, 32, 64]\n",
    "stations = [\"APA\", \"central_park\", \"DEN\", \"water_dept\"]\n",
    "\n",
    "num_epochs = 30\n",
    "ensemble_size = 10\n",
    "loss = np.zeros((len(model_types), \n",
    "                 len(hidden_sizes), \n",
    "                 len(stations), \n",
    "                 ensemble_size, \n",
    "                 num_epochs,\n",
    "                 2)) # training and validation loss for each configuration\n",
    "\n",
    "all_models = {} #store all model ensembles in a dictionary\n",
    "\n",
    "num_models = len(model_types) * len(hidden_sizes) * len(stations) * ensemble_size\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "model_id = 1\n",
    "\n",
    "for m, model_type in enumerate(model_types):\n",
    "    for h, hidden_size in enumerate(hidden_sizes):\n",
    "        for s, station in enumerate(stations):\n",
    "            key = f'{model_type}_{hidden_size}_{station}'\n",
    "            train_loader, validation_loader, test_loader = loaders[station].values()\n",
    "\n",
    "            ensemble = [WeatherRNN(input_size=1,\n",
    "                                   hidden_size=hidden_size,\n",
    "                                   output_size=1,\n",
    "                                   num_layers=1,\n",
    "                                   rnn_type=model_type) for _ in range(ensemble_size)]\n",
    "\n",
    "            # Parallel training\n",
    "            results = Parallel(n_jobs=-1)(\n",
    "                delayed(train_member)(model, train_loader, validation_loader, num_epochs)\n",
    "                for model in ensemble\n",
    "            )\n",
    "\n",
    "            for e, (train_loss, val_loss) in enumerate(results):\n",
    "                loss[m, h, s, e, :, :] = np.stack((train_loss, val_loss), axis=1)\n",
    "            \n",
    "            all_models[key] = ensemble # add model ensemble to dictionary\n",
    "\n",
    "            print(f'{10*model_id}/{num_models} models complete')\n",
    "            model_id += 1\n",
    "\n",
    "                \n",
    "end_time = time.time()\n",
    "print(f'Time Elapsed: {end_time-start_time:.3f}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, ensemble in all_models.items():\n",
    "    model_type, hidden_size, station = key.rsplit(\"_\", maxsplit=2)\n",
    "    for i, model in enumerate(ensemble):\n",
    "        path = f\"{year}_{month}_{date}_model_save/{model_type}_{hidden_size}_{station}_model{i}.pt\"\n",
    "        torch.save(model.state_dict(), path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stat155_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1 (tags/v3.11.1:a7a450f, Dec  6 2022, 19:58:39) [MSC v.1934 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3d24613238e23b2647e576cf7003f7a340e5d0eea2e9f48644ff93fa14dea200"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
